{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e470cd8-840a-4127-b36d-090297fd66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import argparse\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "        get_peft_model, \n",
    "        prepare_model_for_kbit_training, \n",
    "        LoraConfig\n",
    "    )\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4670ff28-27fb-473c-b53f-ec5bdbc147b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cc07e9745f4d2c86f202c628f8b15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            load_in_8bit=True,\n",
    "                                            device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bb3669-59fc-4844-978f-d31948567a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['publicationDate', 'title', 'abstract'],\n",
      "    num_rows: 32654\n",
      "})\n",
      "Dataset({\n",
      "    features: ['title', 'abstract', 'keyword'],\n",
      "    num_rows: 278\n",
      "})\n",
      "The abstract of the paper:\n",
      " Magnetically tuned Ni_0.3Co_0.7Dy_xFe_2–xO_4 ferrites for high-density data storage applications\n",
      " Abstract: Dysprosium (Dy^3+)-substituted Ni–Co nanoparticles were synthesized by sol–gel technique. Structural and morphological analyses were accomplished by X-ray diffraction (XRD), scanning electron microscopy (SEM) and field emission transmission electron microscopy (FE-TEM). The crystallite size and lattice parameter followed a decreasing trend up on increase in Dy^3+ substitution for the concentration x  ≤ 0.15, which is due to the hindrance in crystallite growth and deposition of Dy^3+ on grain boundaries, respectively. The lattice strain was increased from 5.027 to 8.814 × $${10}^{-3}$$ 10 - 3 with enhancement in Dy^3+ content. The morphological studies showed uniform distribution of particles with slight agglomeration and the average particle size was calculated to be 22.17 nm, which is in good agreement with XRD results. The magnetic studies were executed by vibrating sample magnetometer (VSM) over a wide range of applied magnetic field. The soft ferrimagnetic nature of these ferrites was revealed by narrow (M–H) curve. The magnetic parameters exhibited decreasing behavior upon increasing amount of substitution. The coercivity ( H _ c ) was recorded to be 1097 Oe for x  = 0.00 and saturation magnetization ( M _ s ) was calculated in the range 27.04–40.86 emu/g. The anisotropy constant and magneton number were found to be in the range of 9887–46,703 erg/cm^3 and 1.21–1.71 µ_B, respectively. These properties of prepared ferrites point towards their applicability in magnetic recording instruments, memory, and high-density data storage devices. </s> \n"
     ]
    }
   ],
   "source": [
    "# Load your custom JSON dataset\n",
    "custom_data = load_dataset('json', data_files='selected_data.json')\n",
    "\n",
    "# Access train, test, and validation splits if available\n",
    "data_train = custom_data['train']\n",
    "\n",
    "custom_data = load_dataset('json', data_files='data.json')\n",
    "data_val = custom_data['train']\n",
    "\n",
    "# Print the dataset details\n",
    "print(data_train)\n",
    "print(data_val)\n",
    "\n",
    "# Access an example\n",
    "#example = data_train[0]\n",
    "#print(example)\n",
    "\n",
    "def generate_prompt(title, abstract=None, eos_token=\"</s>\"):\n",
    "  instruction = \"The abstract of the paper:\\n\"\n",
    "  input = f\"{title}\\n\"\n",
    "  abstract = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, abstract])\n",
    "  return prompt\n",
    "\n",
    "print(generate_prompt(data_train[0][\"title\"], data_train[0][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9b843b-457f-4aea-b951-5f37c16971c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract of the paper:\n",
      " Characteristic Research of Magnetic Controllable Voltage Regulator\n",
      " Abstract:   In this paper, we design a magnetic controllable voltage regulator. It is composed by two parts, one part is the control circuit and another part is the power circuit. The control circuit is made up of an LC oscillator, a comparator and some passive components. When the output voltage of the LC oscillator exceeds the reference voltage, the comparator will turn on the MOSFET to cut off the current flowing through it. And when the output voltage of the LC oscillator drops below the reference voltage, the comparator turns off the MOSFET to let the current flow through it again. The power circuit is made up of a transformer with three windings, which are the primary winding, secondary winding and tertiary winding respectively. The primary winding is connected in series with the load. The secondary winding is used for generating the high frequency signal. The tertiary winding is used as the feedback loop. The main advantage of our proposed method is that it can be applied to different kinds of loads without changing its structure. Besides, there is no need to change any component or adjust any parameter during the process of operation. Finally, we test the performance of our proposed method under various conditions. Experimental results show that the proposed method has good stability and robustness.\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[50][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f33fd7-008c-4d74-82d0-4814b21b696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=256,\n",
    "        lora_alpha=512,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "# this should be set for finutning and batched inference\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Loading in 8 bit ...\"\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c917622f-73a7-4e62-80ab-467ca0754ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PeftModelForCausalLM(\n",
      "      (base_model): LoraModel(\n",
      "        (model): LlamaForCausalLM(\n",
      "          (model): LlamaModel(\n",
      "            (embed_tokens): Embedding(32001, 4096)\n",
      "            (layers): ModuleList(\n",
      "              (0-31): 32 x LlamaDecoderLayer(\n",
      "                (self_attn): LlamaAttention(\n",
      "                  (q_proj): lora.Linear8bitLt(\n",
      "                    (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=256, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (k_proj): lora.Linear8bitLt(\n",
      "                    (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=256, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v_proj): lora.Linear8bitLt(\n",
      "                    (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=256, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (o_proj): lora.Linear8bitLt(\n",
      "                    (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.05, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=256, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (rotary_emb): LlamaRotaryEmbedding()\n",
      "                )\n",
      "                (mlp): LlamaMLP(\n",
      "                  (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "                  (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
      "                  (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
      "                  (act_fn): SiLU()\n",
      "                )\n",
      "                (input_layernorm): LlamaRMSNorm()\n",
      "                (post_attention_layernorm): LlamaRMSNorm()\n",
      "              )\n",
      "            )\n",
      "            (norm): LlamaRMSNorm()\n",
      "          )\n",
      "          (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9981f016-a05d-4ea9-a7f2-1c8f69f1597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"cp\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "per_device_eval_batch_size = 4\n",
    "eval_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 200\n",
    "logging_steps = 200\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 0.2\n",
    "#max_steps = 50\n",
    "warmup_ratio = 0.03\n",
    "evaluation_strategy=\"epoch\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            logging_steps=logging_steps,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            #max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49676da0-6985-4bcb-a80f-b278b283c074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa36c0c5a7e41ae9aedc02dd08af26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2383540a8fcf4a3b8b775cb070b6e7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 08:49, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.098400</td>\n",
       "      <td>1.891765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.891100</td>\n",
       "      <td>1.811098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.814300</td>\n",
       "      <td>1.744567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.705700</td>\n",
       "      <td>1.667625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>1.575743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for d, s in zip(prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54db31b-59b9-473f-9265-4142818dfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model_id = \"cp/checkpoint-1000\"\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id, torch_dtype=torch.float16, offload_folder=\"lora_results/lora_7/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f73bb9-2a2c-4f20-a8b1-537889ea2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract of the paper:\n",
      " Characteristic Research of Magnetic Controllable Voltage Regulator\n",
      " Abstract:   In this paper, a magnetic controllable voltage regulator is designed. It consists of a transformer and two inductors. The control circuit includes an auxiliary winding on the primary side of the transformer and a current-controlled switch on the secondary side. When the load changes, the output voltage can be adjusted by changing the duty cycle of the switch. By using the PID controller to control the duty cycle of the switch, the output voltage can be controlled accurately and\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[50][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)\n",
    "# Summarize the following:\n",
    "#  Pitt: Hey Teddy! Have you received my message?\n",
    "# Teddy: No. An email?\n",
    "# Pitt: No. On the FB messenger.\n",
    "# Teddy: Let me check.\n",
    "# Teddy: Yeah. Ta!\n",
    "#  Summary:   Pitt sent a message to Teddy on Facebook Messenger, but he didn't receive it yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b1b0f0-b8db-4e73-bd5d-ef09466dbf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should be set for finutning and batched inference\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee3021-a7c8-419c-8174-b55fc182547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in 8 bit ...\"\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "#model = get_peft_model(model, lora_config)\n",
    "\n",
    "output_dir = \"cp\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "per_device_eval_batch_size = 4\n",
    "eval_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 200\n",
    "logging_steps = 200\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 0.2\n",
    "#max_steps = 50\n",
    "warmup_ratio = 0.03\n",
    "evaluation_strategy=\"epoch\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            logging_steps=logging_steps,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            #max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        )\n",
    "\n",
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for d, s in zip(prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    #peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1669c18-e142-4baf-893e-5f0c9f1827d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
