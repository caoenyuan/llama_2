{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a8afe9-1f35-48de-8384-6063d350cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340eccc8-60ac-4b8c-9807-dcbbc86185e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7cabcb12f448999fbbc695ac105256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9148cc-49d4-4e74-bc10-1a8ef0788108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e429f5582f410b8144e7e6dfb7252f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             load_in_8bit=True,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('enyuan/llama')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('enyuan/llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f68fc9-962f-4f5f-8c74-739fb4e2dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'abstract', 'publicationDate', 'doi'],\n",
      "    num_rows: 165071\n",
      "})\n",
      "Dataset({\n",
      "    features: ['title', 'abstract', 'publicationDate'],\n",
      "    num_rows: 559\n",
      "})\n",
      "The abstract of the paper:\n",
      " Inconel 625 sustainable milling surface integrity and the dependence on alloy processing route\n",
      " Abstract: The discovery of deepwater oil and gas sources has altered the scenario of world production of oil products, attracting even more attention to nickel superalloys. However, this class of materials can be used in several applications. Furthermore, nickel superalloys are highly dependent on their processing history, and the manner in which superalloys react to machining can directly affect the finished product. This work aims to evaluate the surface integrity of two different materials after cryogenic side-milling in conditions that stimulate severe plastic deformation (SPD) and high heat generation. The results show that the material response to machining depends strongly on the pre-processing route instead of most assumptions. While cryogenic cooling led to significant sub-surface hardness and microstructural changes in wrought Inconel 625 alloy, such changes were not observed for clad Inconel 625. Therefore, in order to achieve significant surface integrity changes, process parameters need to be selected and optimized accordingly. Also, the findings indicate that some new factors established significant affect/change surface integrity: (a) SPD through a high r_β/h ratio; (b) the specific pre-processing thermomechanical history of the workpiece material; and (c) and cryogenic cooling, by changing material properties, reducing temperature and altering cutting phenomena and chip formation. </s> \n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"enyuan/Abstracts\")\n",
    "data_train = data[\"train\"]\n",
    "\n",
    "custom_data = load_dataset('json', data_files='data_eval.json')\n",
    "data_val = custom_data['train']\n",
    "\n",
    "# Print the dataset details\n",
    "print(data_train)\n",
    "print(data_val)\n",
    "\n",
    "# Access an example\n",
    "#example = data_train[0]\n",
    "#print(example)\n",
    "\n",
    "def generate_prompt(title, abstract=None, eos_token=\"</s>\"):\n",
    "  instruction = \"The abstract of the paper:\\n\"\n",
    "  input = f\"{title}\\n\"\n",
    "  abstract = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, abstract])\n",
    "  return prompt\n",
    "\n",
    "print(generate_prompt(data_train[0][\"title\"], data_train[0][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347f25a1-b25f-40a6-9f81-4c0bdcfe8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract of the paper:\n",
      " Effect of cryogenic cooling on residual stresses and surface finish of 316L during hybrid manufacturing\n",
      " Abstract:   In this work, the effect of cryogenic cooling (CC) on residual stress and surface roughness is investigated in a hybrid additive manufactured part. A 3D printed stainless steel (SS) component was subjected to CC treatment using liquid nitrogen (LN2). Residual stress measurements were carried out by X-ray diffraction method and surface roughness analysis was performed with white light interferometry technique. The results show that the residual stress profile decreases significantly after CC treatment. Surface roughness also reduces considerably due to the removal of oxide layer formed at the interface between LN2 and SS. This study demonstrates that CC can be used as an effective post processing step for improving mechanical properties and surface quality of AM parts.\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[50][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70887e88-ddc1-44b4-848b-fd554027b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('materials.txt', 'r') as file:\n",
    "    word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b263087-4b39-4559-a1ca-fc0db0117772",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'title': word_list,\n",
    "    'abstract': [s.replace('_', '') for s in word_list],\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])\n",
    "\n",
    "new_data = {\n",
    "    'title': [s.replace('_', '') for s in word_list],\n",
    "    'abstract': word_list,\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a51116-f6ef-4701-939f-d07933fb8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_train.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644d9dfc-ad91-4d3e-bb14-2ee5600631c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The material :\n",
      " NiFeAlO4 is NiFeAlO_4 </s> \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(type, title, abstract=None, eos_token=\"</s>\"):\n",
    "    if type == 'material':\n",
    "        instruction = \"The material :\\n\"\n",
    "        input = f\"{title} is\"\n",
    "        output = f\"{abstract + ' ' + eos_token if abstract else ''} \"\n",
    "        prompt = (\" \").join([instruction, input, output])\n",
    "    else:\n",
    "        instruction = \"The abstract of the paper:\\n\"\n",
    "        input = f\"{title}\\n\"\n",
    "        output = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "        prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data_train[-1][\"doi\"], data_train[-1][\"title\"], data_train[-1][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a4ce5ac-170a-42c2-8b05-b204b6bf9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bb07ccc-4f63-44d0-8e25-16ad278ff7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5552 tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add new tokens to the tokenizer\n",
    "num_added_toks = tokenizer.add_tokens(word_list)\n",
    "print(f\"Added {num_added_toks} tokens\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f119b5df-ac5a-46f8-be81-b12943b162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Freeze all parameters in the model\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# Enable gradient updates for the entire embedding layer\n",
    "# Assuming you might want to fine-tune all embeddings, but here's how to selectively unfreeze\n",
    "embeddings.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8483b140-a7e6-4175-8e01-6e3f13879301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',            # 输出目录\n",
    "    num_train_epochs=2,              # 总训练轮数\n",
    "    per_device_train_batch_size=4,   # 训练的batch size\n",
    "    per_device_eval_batch_size=4,    # 验证的batch size\n",
    "    gradient_accumulation_steps=4, \n",
    "    #gradient_checkpointing=True,\n",
    "    #optim = \"paged_adamw_32bit\",\n",
    "    optim = \"adamw_torch\",\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    warmup_steps=300,                # 预热步数\n",
    "    learning_rate = 1e-4,\n",
    "    max_grad_norm = 0.2,\n",
    "    #max_steps = 50,\n",
    "    #warmup_ratio = 0.03,\n",
    "    #weight_decay=0.01,               # 权重衰减\n",
    "    save_strategy=\"steps\",           # 设置保存策略为\"steps\"\n",
    "    save_steps=300,                  # 每500步保存一次模型\n",
    "    save_total_limit=3,              # 最多保存3个检查点\n",
    "    evaluation_strategy=\"epoch\",     # 设置评估策略为\"steps\"\n",
    "    group_by_length=True,\n",
    "    #eval_steps=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e64ad6-36bd-4b68-b995-bfef056bb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checkpointing enabling\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49402b-1027-4e8b-a777-54b6b03f7966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c82be8d0114ea3bf9a23126c2e85f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/198383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2b633340d74572b8d7f22660d971a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='24798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   76/24798 05:16 < 29:22:19, 0.23 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for a, d, s in zip(prompt[\"doi\"], prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(a, d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c50c387-2084-4e3b-b5d6-6ac9a6d15854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low magnetic damping constant.\n",
      " The molecular formula of the material:\n",
      "Word: BaCo_xNi_2-xFe_16O_27, Probability: 0.0457\n",
      "Word: K_2NaNiF_6, Probability: 0.0281\n",
      "Word: C, Probability: 0.0275\n",
      "Word: CoZnTiO_4, Probability: 0.0215\n",
      "Word: Mg_10Fe, Probability: 0.0193\n",
      "Word: \n",
      ", Probability: 0.0189\n",
      "Word: Ca_2Fe_2-xSc_xO_5, Probability: 0.0139\n",
      "Word: Ca_3Fe_15O_25, Probability: 0.0133\n",
      "Word: Fe, Probability: 0.0124\n",
      "Word: Ag_2FeSn_3S_8, Probability: 0.0115\n",
      "Word: Fe_20_4, Probability: 0.0104\n",
      "Word: Ni_0.9Zn_0.1CoO_2, Probability: 0.0092\n",
      "Word: V_2O_3, Probability: 0.0080\n",
      "Word: La_0.7Ca_0.2-xSr_xK_0.1MnO_3, Probability: 0.0078\n",
      "Word: (, Probability: 0.0063\n",
      "Word: Ni_0.91Pd_0.09, Probability: 0.0061\n",
      "Word: Ta_0.1CoSb, Probability: 0.0054\n",
      "Word: La_0.7Sr_0.3Mn_0.9Fe_0.05Mg_0.05O_3, Probability: 0.0053\n",
      "Word: Cu_0.5CrCo_1.5S_4, Probability: 0.0052\n",
      "Word: M, Probability: 0.0052\n",
      "Word: Sr_0.08MnO_3, Probability: 0.0049\n",
      "Word: Cu, Probability: 0.0049\n",
      "Word: Co_13, Probability: 0.0049\n",
      "Word: Fe_2MnGe, Probability: 0.0045\n",
      "Word: LiCoV_3O_12, Probability: 0.0043\n",
      "Word: Ni_0.7Cu_0.1Zn_0.2La, Probability: 0.0042\n",
      "Word: Pb_2FeWO_6, Probability: 0.0042\n",
      "Word: BaFe_12, Probability: 0.0041\n",
      "Word: Ba_2CoTaO_6, Probability: 0.0040\n",
      "Word: Co_0.0014Ni_0.65Fe_2.34O_4..01, Probability: 0.0038\n",
      "Word: , Probability: 0.0037\n",
      "Word: Ba_3Mn_2Fe_24O_41, Probability: 0.0037\n",
      "Word: TiFe_0.3Ni_0.8Sb, Probability: 0.0037\n",
      "Word: Y_3Fe_5O_12, Probability: 0.0036\n",
      "Word: Fe_2MnSi, Probability: 0.0036\n",
      "Word: Si, Probability: 0.0036\n",
      "Word: LnSrFeO_4, Probability: 0.0036\n",
      "Word: Zn_0.25Co_0.75Fe_2O_4, Probability: 0.0035\n",
      "Word: VCo_2Al, Probability: 0.0034\n",
      "Word: N, Probability: 0.0033\n",
      "Word: BaTi_0.95Fe_0.05O_3, Probability: 0.0033\n",
      "Word: CoO_4, Probability: 0.0032\n",
      "Word: Tb_1.90Ni_0.10O_3, Probability: 0.0032\n",
      "Word: NdFe_12, Probability: 0.0032\n",
      "Word: MgIn_2-xMn_xO_4, Probability: 0.0031\n",
      "Word: Ti_2NiH_0.8, Probability: 0.0031\n",
      "Word: Ba_2V_3O_9, Probability: 0.0031\n",
      "Word: Zn_2VO_4, Probability: 0.0031\n",
      "Word: Co_0.2Zn_0.8Fe_2O_4, Probability: 0.0030\n",
      "Word: LiFePO_4F, Probability: 0.0030\n",
      "Word: Cu_0.5Fe_1.25Li_0.25GaO_4, Probability: 0.0030\n",
      "Word: ., Probability: 0.0029\n",
      "Word: FeGa_2Se_4, Probability: 0.0029\n",
      "Word: MgV_2O_4, Probability: 0.0029\n",
      "Word: Ni_3InB_0.5, Probability: 0.0028\n",
      "Word: Ca_2FeAlO_5, Probability: 0.0028\n",
      "Word: Ni_55.7Ti, Probability: 0.0027\n",
      "Word: Sr_2FeSbO_6, Probability: 0.0027\n",
      "Word: CH, Probability: 0.0026\n",
      "Word: CsV_3Sb_5, Probability: 0.0026\n",
      "Word: Co_49Ni_21Ga_30, Probability: 0.0026\n",
      "Word: H, Probability: 0.0026\n",
      "Word: Mn_3-xFe_xO_4, Probability: 0.0026\n",
      "Word: RCo_5, Probability: 0.0026\n",
      "Word: Zr_2MnAl, Probability: 0.0025\n",
      "Word: Sr_1.52Eu_0.48VO_3.948, Probability: 0.0025\n",
      "Word: PbCo_0.5Sn_0.5Ho, Probability: 0.0025\n",
      "Word: MnLu_2S_4, Probability: 0.0025\n",
      "Word: Fe_2Al_5, Probability: 0.0024\n",
      "Word: A, Probability: 0.0024\n",
      "Word: Fe_2Mn_1Co_0.5, Probability: 0.0024\n",
      "Word: Dy_2Fe_10Al_7, Probability: 0.0024\n",
      "Word: LaFe_11ZnO_19, Probability: 0.0024\n",
      "Word: ACo_2O_4, Probability: 0.0024\n",
      "Word: BaTi_1-xNi_xO_3, Probability: 0.0023\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0023\n",
      "Word: Pb_9Ca_5MnSi_9O_33, Probability: 0.0023\n",
      "Word: Ca_0.2MnO_3, Probability: 0.0022\n",
      "Word: K_xNa_1-xFe_7O_11, Probability: 0.0022\n",
      "Word: Co_2TiAl, Probability: 0.0022\n",
      "Word: Co_3GaN, Probability: 0.0022\n",
      "Word: MX_2Fe_4O_11, Probability: 0.0022\n",
      "Word: Fe_3S_4, Probability: 0.0021\n",
      "Word: Co_xZn_7-xSb_2O_12, Probability: 0.0021\n",
      "Word: Ca, Probability: 0.0021\n",
      "Word: LiCoTiO_4, Probability: 0.0020\n",
      "Word: Gd_0.1Fe_1.9O_4, Probability: 0.0020\n",
      "Word: CuMnO_6.5, Probability: 0.0020\n",
      "Word: Pr_0.7Ba_0.3CoO_3, Probability: 0.0020\n",
      "Word: Bi_25FeO_40, Probability: 0.0020\n",
      "Word: Hf_2VZ, Probability: 0.0020\n",
      "Word: Cd_1-xFe_1, Probability: 0.0019\n",
      "Word: NiNH_2SO_3, Probability: 0.0019\n",
      "Word: Ni, Probability: 0.0019\n",
      "Word: Ni_48Mn_36Sn_14Pd_2, Probability: 0.0019\n",
      "Word: NiO_3, Probability: 0.0018\n",
      "Word: [, Probability: 0.0018\n",
      "Word: CoB_7, Probability: 0.0018\n",
      "Word: Ce_1-xY_xOFeAs, Probability: 0.0018\n",
      "Word: Zn_0.60Ni_0.40Fe_2O_4, Probability: 0.0018\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetic damping constant.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 100\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a28fd5c-f37e-495a-a6d8-d1efb6246cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low magnetocrystalline anisotropy.\n",
      " The molecular formula of the material:\n",
      "Word: BaCo_xNi_2-xFe_16O_27, Probability: 0.0453\n",
      "Word: Ca_2Fe_2-xSc_xO_5, Probability: 0.0321\n",
      "Word: K_2NaNiF_6, Probability: 0.0290\n",
      "Word: \n",
      ", Probability: 0.0270\n",
      "Word: C, Probability: 0.0196\n",
      "Word: Ca_3Fe_15O_25, Probability: 0.0171\n",
      "Word: CoZnTiO_4, Probability: 0.0152\n",
      "Word: Fe, Probability: 0.0123\n",
      "Word: Mg_10Fe, Probability: 0.0111\n",
      "Word: Fe_20_4, Probability: 0.0103\n",
      "Word: Ag_2FeSn_3S_8, Probability: 0.0092\n",
      "Word: Ta_0.1CoSb, Probability: 0.0080\n",
      "Word: Sr_0.08MnO_3, Probability: 0.0074\n",
      "Word: V_2O_3, Probability: 0.0073\n",
      "Word: (, Probability: 0.0068\n",
      "Word: Ni_0.9Zn_0.1CoO_2, Probability: 0.0068\n",
      "Word: Co_13, Probability: 0.0064\n",
      "Word: Ni_0.7Cu_0.1Zn_0.2La, Probability: 0.0062\n",
      "Word: M, Probability: 0.0058\n",
      "Word: Cu, Probability: 0.0054\n",
      "Word: Ni_0.91Pd_0.09, Probability: 0.0053\n",
      "Word: LnSrFeO_4, Probability: 0.0053\n",
      "Word: Cu_0.5CrCo_1.5S_4, Probability: 0.0053\n",
      "Word: Zn_0.25Co_0.75Fe_2O_4, Probability: 0.0049\n",
      "Word: Ni_48Mn_36Sn_14Pd_2, Probability: 0.0046\n",
      "Word: VCo_2Al, Probability: 0.0046\n",
      "Word: TiFe_0.3Ni_0.8Sb, Probability: 0.0044\n",
      "Word: MgV_2O_4, Probability: 0.0043\n",
      "Word: La_0.7Sr_0.3Mn_0.9Fe_0.05Mg_0.05O_3, Probability: 0.0041\n",
      "Word: MgIn_2-xMn_xO_4, Probability: 0.0040\n",
      "Word: Zn_2VO_4, Probability: 0.0040\n",
      "Word: Ba_3Mn_2Fe_24O_41, Probability: 0.0039\n",
      "Word: Tb_1.90Ni_0.10O_3, Probability: 0.0038\n",
      "Word: Sr_1.52Eu_0.48VO_3.948, Probability: 0.0038\n",
      "Word: , Probability: 0.0038\n",
      "Word: Dy_2Fe_10Al_7, Probability: 0.0037\n",
      "Word: Ba_2CoTaO_6, Probability: 0.0037\n",
      "Word: ., Probability: 0.0037\n",
      "Word: Pb_2FeWO_6, Probability: 0.0035\n",
      "Word: La_0.7Ca_0.2-xSr_xK_0.1MnO_3, Probability: 0.0035\n",
      "Word: LiCoV_3O_12, Probability: 0.0034\n",
      "Word: Co_0.0014Ni_0.65Fe_2.34O_4..01, Probability: 0.0034\n",
      "Word: CoO_4, Probability: 0.0033\n",
      "Word: Si, Probability: 0.0033\n",
      "Word: Y_3Fe_5O_12, Probability: 0.0033\n",
      "Word: Pb_9Ca_5MnSi_9O_33, Probability: 0.0033\n",
      "Word: Fe_2MnGe, Probability: 0.0032\n",
      "Word: Zn_0.60Ni_0.40Fe_2O_4, Probability: 0.0032\n",
      "Word: FeGa_2Se_4, Probability: 0.0032\n",
      "Word: Co_2TiAl, Probability: 0.0030\n",
      "Word: Fe_2MnSi, Probability: 0.0030\n",
      "Word: Mn_3-xFe_xO_4, Probability: 0.0029\n",
      "Word: Co_0.2Zn_0.8Fe_2O_4, Probability: 0.0029\n",
      "Word: NiNH_2SO_3, Probability: 0.0027\n",
      "Word: [, Probability: 0.0027\n",
      "Word: PbCo_0.5Sn_0.5Ho, Probability: 0.0027\n",
      "Word: N, Probability: 0.0027\n",
      "Word: Ca, Probability: 0.0027\n",
      "Word: Fe_3S_4, Probability: 0.0026\n",
      "Word: H, Probability: 0.0026\n",
      "Word: Cu_0.5Fe_1.25Li_0.25GaO_4, Probability: 0.0026\n",
      "Word: Zn_0.9Co_0.1O_1.033, Probability: 0.0026\n",
      "Word: LiCoTiO_4, Probability: 0.0026\n",
      "Word: LiNi_0.65-xCo_0.1Mn_0.25Cr_xO_2, Probability: 0.0025\n",
      "Word: A, Probability: 0.0025\n",
      "Word: BaTi_0.95Fe_0.05O_3, Probability: 0.0024\n",
      "Word: CH, Probability: 0.0024\n",
      "Word: CsV_3Sb_5, Probability: 0.0023\n",
      "Word: LiZn_0.5Mn_1.5O_4, Probability: 0.0023\n",
      "Word: Ca_2Mn_2O_5, Probability: 0.0023\n",
      "Word: BaFe_12, Probability: 0.0023\n",
      "Word: Zn_0.6Fe_2O_4, Probability: 0.0023\n",
      "Word: ACo_2O_4, Probability: 0.0022\n",
      "Word: MnLu_2S_4, Probability: 0.0022\n",
      "Word: Co_49Ni_21Ga_30, Probability: 0.0022\n",
      "Word: Ni_21Nb_2B_6, Probability: 0.0022\n",
      "Word: Fe_xCd_1-xCr_2S_4, Probability: 0.0022\n",
      "Word: LiFePO_4F, Probability: 0.0021\n",
      "Word: Co_3GaN, Probability: 0.0021\n",
      "Word: Cd_1-xFe_1, Probability: 0.0021\n",
      "Word: NdFe_12, Probability: 0.0021\n",
      "Word: NiO_3, Probability: 0.0021\n",
      "Word: Fe_2Mn_1Co_0.5, Probability: 0.0021\n",
      "Word: MX_2Fe_4O_11, Probability: 0.0021\n",
      "Word: Fe_2Al_5, Probability: 0.0021\n",
      "Word: Zr_2MnAl, Probability: 0.0020\n",
      "Word: Ni_3InB_0.5, Probability: 0.0020\n",
      "Word: CoO_2, Probability: 0.0020\n",
      "Word: BaTi_1-xNi_xO_3, Probability: 0.0020\n",
      "Word: Pr_0.7Ba_0.3CoO_3, Probability: 0.0019\n",
      "Word: BaFeC_3, Probability: 0.0019\n",
      "Word: LaFe_11ZnO_19, Probability: 0.0019\n",
      "Word: Ce_1-xY_xOFeAs, Probability: 0.0019\n",
      "Word: X, Probability: 0.0018\n",
      "Word: Cd_1-xMn_xTe, Probability: 0.0018\n",
      "Word: RMnO_3, Probability: 0.0018\n",
      "Word: Sr_2FeSbO_6, Probability: 0.0018\n",
      "Word: Ni_55.7Ti, Probability: 0.0018\n",
      "Word: Bi_25FeO_40, Probability: 0.0018\n",
      "Word: Al, Probability: 0.0018\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetocrystalline anisotropy.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 100\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4641f4-1991-454a-b83f-af5c44bb4ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low density of states at the Fermi level.\n",
      " The molecular formula of the material:\n",
      "Word: BaCo_xNi_2-xFe_16O_27, Probability: 0.0439\n",
      "Word: Ca_2Fe_2-xSc_xO_5, Probability: 0.0306\n",
      "Word: \n",
      ", Probability: 0.0266\n",
      "Word: C, Probability: 0.0232\n",
      "Word: Ca_3Fe_15O_25, Probability: 0.0150\n",
      "Word: K_2NaNiF_6, Probability: 0.0141\n",
      "Word: Mg_10Fe, Probability: 0.0118\n",
      "Word: Fe_20_4, Probability: 0.0092\n",
      "Word: Sr_0.08MnO_3, Probability: 0.0090\n",
      "Word: Zn_0.25Co_0.75Fe_2O_4, Probability: 0.0086\n",
      "Word: LnSrFeO_4, Probability: 0.0085\n",
      "Word: CoZnTiO_4, Probability: 0.0080\n",
      "Word: Ta_0.1CoSb, Probability: 0.0076\n",
      "Word: La_0.7Ca_0.2-xSr_xK_0.1MnO_3, Probability: 0.0071\n",
      "Word: MgV_2O_4, Probability: 0.0065\n",
      "Word: Ni_0.9Zn_0.1CoO_2, Probability: 0.0062\n",
      "Word: Ag_2FeSn_3S_8, Probability: 0.0056\n",
      "Word: Zn_2VO_4, Probability: 0.0055\n",
      "Word: (, Probability: 0.0054\n",
      "Word: Ni_0.91Pd_0.09, Probability: 0.0051\n",
      "Word: Co_0.2Zn_0.8Fe_2O_4, Probability: 0.0049\n",
      "Word: Ba_2CoTaO_6, Probability: 0.0049\n",
      "Word: BaFe_12, Probability: 0.0049\n",
      "Word: Dy_2Fe_10Al_7, Probability: 0.0046\n",
      "Word: Pb_2FeWO_6, Probability: 0.0044\n",
      "Word: VCo_2Al, Probability: 0.0043\n",
      "Word: V_2O_3, Probability: 0.0041\n",
      "Word: CoO_4, Probability: 0.0041\n",
      "Word: Co_0.0014Ni_0.65Fe_2.34O_4..01, Probability: 0.0041\n",
      "Word: Si, Probability: 0.0041\n",
      "Word: Fe_2MnSi, Probability: 0.0041\n",
      "Word: ., Probability: 0.0040\n",
      "Word: MgIn_2-xMn_xO_4, Probability: 0.0039\n",
      "Word: Ni_0.7Cu_0.1Zn_0.2La, Probability: 0.0039\n",
      "Word: BaTi_0.95Fe_0.05O_3, Probability: 0.0035\n",
      "Word: LiCoV_3O_12, Probability: 0.0035\n",
      "Word: Ni_3InB_0.5, Probability: 0.0035\n",
      "Word: H, Probability: 0.0035\n",
      "Word: Fe_xCd_1-xCr_2S_4, Probability: 0.0035\n",
      "Word: TiFe_0.3Ni_0.8Sb, Probability: 0.0034\n",
      "Word: Fe, Probability: 0.0034\n",
      "Word: CsV_3Sb_5, Probability: 0.0034\n",
      "Word: Co_2TiAl, Probability: 0.0034\n",
      "Word: N, Probability: 0.0033\n",
      "Word: Ce_1-xY_xOFeAs, Probability: 0.0033\n",
      "Word: MX_2Fe_4O_11, Probability: 0.0033\n",
      "Word: Fe_2Al_5, Probability: 0.0032\n",
      "Word: CH, Probability: 0.0032\n",
      "Word: Pb_9Ca_5MnSi_9O_33, Probability: 0.0032\n",
      "Word: Cu_0.5CrCo_1.5S_4, Probability: 0.0032\n",
      "Word: M, Probability: 0.0032\n",
      "Word: Y_3Fe_5O_12, Probability: 0.0032\n",
      "Word: Fe_3S_4, Probability: 0.0031\n",
      "Word: Sr_1.52Eu_0.48VO_3.948, Probability: 0.0030\n",
      "Word: Co_xZn_7-xSb_2O_12, Probability: 0.0030\n",
      "Word: Cu, Probability: 0.0030\n",
      "Word: , Probability: 0.0029\n",
      "Word: Ti_2NiH_0.8, Probability: 0.0029\n",
      "Word: Ba_3Mn_2Fe_24O_41, Probability: 0.0028\n",
      "Word: Co_xZn_3-xNi_4Sb_2O_12, Probability: 0.0027\n",
      "Word: Ca, Probability: 0.0027\n",
      "Word: Eu_2MnWO_6, Probability: 0.0027\n",
      "Word: Ni_48Mn_36Sn_14Pd_2, Probability: 0.0026\n",
      "Word: Fe_2MnGe, Probability: 0.0026\n",
      "Word: RCo_5, Probability: 0.0026\n",
      "Word: NiNH_2SO_3, Probability: 0.0026\n",
      "Word: Zn_0.9Co_0.1O_1.033, Probability: 0.0025\n",
      "Word: La_0.7Sr_0.3Mn_0.9Fe_0.05Mg_0.05O_3, Probability: 0.0025\n",
      "Word: Co_13, Probability: 0.0025\n",
      "Word: LiCoTiO_4, Probability: 0.0025\n",
      "Word: BaTi_1-xNi_xO_3, Probability: 0.0024\n",
      "Word: Tb_1.90Ni_0.10O_3, Probability: 0.0023\n",
      "Word: Ca_2Mn_2O_5, Probability: 0.0023\n",
      "Word: Bi_25FeO_40, Probability: 0.0023\n",
      "Word: Na, Probability: 0.0022\n",
      "Word: NdFe_12, Probability: 0.0022\n",
      "Word: Cu_0.5Fe_1.25Li_0.25GaO_4, Probability: 0.0022\n",
      "Word: Zr_2MnAl, Probability: 0.0022\n",
      "Word: NaMn_7O_12, Probability: 0.0021\n",
      "Word: SrFe_1-xNi_xO_3, Probability: 0.0021\n",
      "Word: La_0.7Sr_0.3Mn_1, Probability: 0.0021\n",
      "Word: Sr_2FeSbO_6, Probability: 0.0021\n",
      "Word: CoL_2, Probability: 0.0020\n",
      "Word: Ni_20Ti_3B_6, Probability: 0.0020\n",
      "Word: Ni_21Nb_2B_6, Probability: 0.0020\n",
      "Word: [, Probability: 0.0020\n",
      "Word: Cu_0.3Ni_0.1Zn_0.6Fe_2O_4, Probability: 0.0020\n",
      "Word: P, Probability: 0.0020\n",
      "Word: EuVO_3, Probability: 0.0019\n",
      "Word: FeGa_2Se_4, Probability: 0.0019\n",
      "Word: Fe_61.37Cr_3.78Co_6.84V_0.85W_0.82Mo_1.06Nb_0.85B_19.87C_1.99Si_2.57, Probability: 0.0019\n",
      "Word: CuMnO_6.5, Probability: 0.0019\n",
      "Word: Ni_5GeB_2O_10, Probability: 0.0019\n",
      "Word: Co_3GaN, Probability: 0.0019\n",
      "Word: Ni_55.7Ti, Probability: 0.0019\n",
      "Word: A, Probability: 0.0019\n",
      "Word: Fe_1.9Pt_0.1As_2, Probability: 0.0018\n",
      "Word: NiO_3, Probability: 0.0018\n",
      "Word: PbCo_0.5Sn_0.5Ho, Probability: 0.0018\n",
      "Word: X, Probability: 0.0018\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low density of states at the Fermi level.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 100\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff7555cd-6b66-4945-900b-e8d6d8ed5ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with high conductivity.\n",
      " The molecular formula of the material:\n",
      "Word: C, Probability: 0.0473\n",
      "Word: \n",
      ", Probability: 0.0428\n",
      "Word: BaCo_xNi_2-xFe_16O_27, Probability: 0.0283\n",
      "Word: CoZnTiO_4, Probability: 0.0204\n",
      "Word: Mg_10Fe, Probability: 0.0177\n",
      "Word: Ca_2Fe_2-xSc_xO_5, Probability: 0.0160\n",
      "Word: Ca_3Fe_15O_25, Probability: 0.0120\n",
      "Word: Fe, Probability: 0.0118\n",
      "Word: Ta_0.1CoSb, Probability: 0.0117\n",
      "Word: K_2NaNiF_6, Probability: 0.0113\n",
      "Word: V_2O_3, Probability: 0.0102\n",
      "Word: (, Probability: 0.0091\n",
      "Word: Cu, Probability: 0.0087\n",
      "Word: Sr_0.08MnO_3, Probability: 0.0076\n",
      "Word: Ag_2FeSn_3S_8, Probability: 0.0073\n",
      "Word: Ni_0.9Zn_0.1CoO_2, Probability: 0.0064\n",
      "Word: Co_0.0014Ni_0.65Fe_2.34O_4..01, Probability: 0.0062\n",
      "Word: M, Probability: 0.0061\n",
      "Word: Pb_2FeWO_6, Probability: 0.0051\n",
      "Word: CH, Probability: 0.0051\n",
      "Word: BaFe_12, Probability: 0.0051\n",
      "Word: , Probability: 0.0050\n",
      "Word: Fe_20_4, Probability: 0.0049\n",
      "Word: P, Probability: 0.0047\n",
      "Word: H, Probability: 0.0046\n",
      "Word: Fe_2Al_5, Probability: 0.0045\n",
      "Word: Ca_0.2MnO_3, Probability: 0.0043\n",
      "Word: La_0.7Sr_0.3Mn_0.9Fe_0.05Mg_0.05O_3, Probability: 0.0042\n",
      "Word: ., Probability: 0.0042\n",
      "Word: N, Probability: 0.0041\n",
      "Word: Tb_1.90Ni_0.10O_3, Probability: 0.0041\n",
      "Word: A, Probability: 0.0040\n",
      "Word: Si, Probability: 0.0040\n",
      "Word: Cu_0.5CrCo_1.5S_4, Probability: 0.0039\n",
      "Word: Ba_2V_3O_9, Probability: 0.0039\n",
      "Word: Ti_2NiH_0.8, Probability: 0.0038\n",
      "Word: Ba_2CoTaO_6, Probability: 0.0036\n",
      "Word: Ca_2FeAlO_5, Probability: 0.0036\n",
      "Word: MgIn_2-xMn_xO_4, Probability: 0.0035\n",
      "Word: La_0.7Ca_0.2-xSr_xK_0.1MnO_3, Probability: 0.0035\n",
      "Word: LiFePO_4F, Probability: 0.0034\n",
      "Word: Al, Probability: 0.0033\n",
      "Word: MX_2Fe_4O_11, Probability: 0.0033\n",
      "Word: [, Probability: 0.0032\n",
      "Word: Ni_0.7Cu_0.1Zn_0.2La, Probability: 0.0032\n",
      "Word: TiFe_0.3Ni_0.8Sb, Probability: 0.0032\n",
      "Word: The, Probability: 0.0032\n",
      "Word: CsV_3Sb_5, Probability: 0.0032\n",
      "Word: Ni_0.91Pd_0.09, Probability: 0.0031\n",
      "Word: Co_13, Probability: 0.0030\n",
      "Word: Pb_9Ca_5MnSi_9O_33, Probability: 0.0030\n",
      "Word: Zn_0.25Co_0.75Fe_2O_4, Probability: 0.0030\n",
      "Word: Eu_2MnWO_6, Probability: 0.0029\n",
      "Word: Co_0.2Zn_0.8Fe_2O_4, Probability: 0.0029\n",
      "Word: Zn_0.9Co_0.1O_1.033, Probability: 0.0029\n",
      "Word: the, Probability: 0.0029\n",
      "Word: Ca, Probability: 0.0028\n",
      "Word: NiO_3, Probability: 0.0028\n",
      "Word: MnLu_2S_4, Probability: 0.0027\n",
      "Word: Zn_2VO_4, Probability: 0.0026\n",
      "Word: Co_49Ni_21Ga_30, Probability: 0.0026\n",
      "Word: Na, Probability: 0.0025\n",
      "Word: X, Probability: 0.0025\n",
      "Word: Sr_1.52Eu_0.48VO_3.948, Probability: 0.0025\n",
      "Word: _, Probability: 0.0025\n",
      "Word: Fe_2Mn_1Co_0.5, Probability: 0.0025\n",
      "Word: Y_3Fe_5O_12, Probability: 0.0024\n",
      "Word: BaTi_0.95Fe_0.05O_3, Probability: 0.0024\n",
      "Word: Sr_2FeSbO_6, Probability: 0.0024\n",
      "Word: Fe_2.16Mn_0.074Ni_0.10O_4, Probability: 0.0023\n",
      "Word: Y_16Fe_78B_6, Probability: 0.0023\n",
      "Word: VCo_2Al, Probability: 0.0023\n",
      "Word: Cu_0.5Fe_1.25Li_0.25GaO_4, Probability: 0.0023\n",
      "Word: MgV_2O_4, Probability: 0.0023\n",
      "Word: LaFe_11ZnO_19, Probability: 0.0023\n",
      "Word: Co_xZn_7-xSb_2O_12, Probability: 0.0023\n",
      "Word: Cu_0.3Ni_0.1Zn_0.6Fe_2O_4, Probability: 0.0022\n",
      "Word: ., Probability: 0.0021\n",
      "Word: Fe_xCd_1-xCr_2S_4, Probability: 0.0021\n",
      "Word: Cd_1-xFe_1, Probability: 0.0021\n",
      "Word: Ni_3InB_0.5, Probability: 0.0021\n",
      "Word: AlNi_2Ti, Probability: 0.0021\n",
      "Word: Fe_2MnSi, Probability: 0.0021\n",
      "Word: AB, Probability: 0.0021\n",
      "Word: Ni_48Mn_36Sn_14Pd_2, Probability: 0.0021\n",
      "Word: Co_3GaN, Probability: 0.0020\n",
      "Word: LiCoV_3O_12, Probability: 0.0020\n",
      "Word: Ni_55.7Ti, Probability: 0.0020\n",
      "Word: Ni_3GeTe_2, Probability: 0.0019\n",
      "Word: Dy_2Fe_10Al_7, Probability: 0.0019\n",
      "Word: LnSrFeO_4, Probability: 0.0019\n",
      "Word: RCo_5, Probability: 0.0018\n",
      "Word: LiZn_0.5Mn_1.5O_4, Probability: 0.0018\n",
      "Word: CoO_4, Probability: 0.0018\n",
      "Word: Co, Probability: 0.0018\n",
      "Word: SrLaFeTaO_6, Probability: 0.0018\n",
      "Word: Ni, Probability: 0.0018\n",
      "Word: SrFe_1-xNi_xO_3, Probability: 0.0018\n",
      "Word: Bi_25FeO_40, Probability: 0.0018\n",
      "Word: Hf_2VZ, Probability: 0.0017\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('high conductivity.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 100\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8b60a1-abbc-4546-af81-4f903108bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Magnetic materials with low magnetic damping constant in low temperture.\n",
      " The molecular formula of the material:\n",
      "Word: BaCo_xNi_2-xFe_16O_27, Probability: 0.0310\n",
      "Word: CoZnTiO_4, Probability: 0.0289\n",
      "Word: K_2NaNiF_6, Probability: 0.0280\n",
      "Word: Mg_10Fe, Probability: 0.0185\n",
      "Word: C, Probability: 0.0166\n",
      "Word: Ni_0.9Zn_0.1CoO_2, Probability: 0.0137\n",
      "Word: Fe, Probability: 0.0132\n",
      "Word: V_2O_3, Probability: 0.0127\n",
      "Word: Ca_3Fe_15O_25, Probability: 0.0120\n",
      "Word: La_0.7Ca_0.2-xSr_xK_0.1MnO_3, Probability: 0.0107\n",
      "Word: Ca_2Fe_2-xSc_xO_5, Probability: 0.0100\n",
      "Word: \n",
      ", Probability: 0.0094\n",
      "Word: La_0.7Sr_0.3Mn_0.9Fe_0.05Mg_0.05O_3, Probability: 0.0089\n",
      "Word: Co_13, Probability: 0.0084\n",
      "Word: Fe_20_4, Probability: 0.0082\n",
      "Word: Ag_2FeSn_3S_8, Probability: 0.0073\n",
      "Word: LiCoV_3O_12, Probability: 0.0064\n",
      "Word: (, Probability: 0.0061\n",
      "Word: MgV_2O_4, Probability: 0.0060\n",
      "Word: Fe_2MnSi, Probability: 0.0058\n",
      "Word: Cu_0.5CrCo_1.5S_4, Probability: 0.0057\n",
      "Word: Ni_0.91Pd_0.09, Probability: 0.0056\n",
      "Word: Fe_2MnGe, Probability: 0.0055\n",
      "Word: Ni_0.7Cu_0.1Zn_0.2La, Probability: 0.0054\n",
      "Word: M, Probability: 0.0053\n",
      "Word: BaTi_0.95Fe_0.05O_3, Probability: 0.0051\n",
      "Word: Sr_0.08MnO_3, Probability: 0.0050\n",
      "Word: Zn_0.25Co_0.75Fe_2O_4, Probability: 0.0050\n",
      "Word: Pb_2FeWO_6, Probability: 0.0049\n",
      "Word: Ta_0.1CoSb, Probability: 0.0048\n",
      "Word: BaTi_1-xNi_xO_3, Probability: 0.0046\n",
      "Word: CuMnO_6.5, Probability: 0.0044\n",
      "Word: Y_3Fe_5O_12, Probability: 0.0043\n",
      "Word: Cu_0.5Fe_1.25Li_0.25GaO_4, Probability: 0.0041\n",
      "Word: PbCo_0.5Sn_0.5Ho, Probability: 0.0040\n",
      "Word: Zn_2VO_4, Probability: 0.0040\n",
      "Word: BaFe_12, Probability: 0.0039\n",
      "Word: LiFePO_4F, Probability: 0.0038\n",
      "Word: RMnO_3, Probability: 0.0037\n",
      "Word: Cu, Probability: 0.0036\n",
      "Word: Zr_2MnAl, Probability: 0.0036\n",
      "Word: Bi_25FeO_40, Probability: 0.0036\n",
      "Word: TiFe_0.3Ni_0.8Sb, Probability: 0.0036\n",
      "Word: Sr_2FeSbO_6, Probability: 0.0034\n",
      "Word: Mn_3-xFe_xO_4, Probability: 0.0034\n",
      "Word: VCo_2Al, Probability: 0.0034\n",
      "Word: LnSrFeO_4, Probability: 0.0032\n",
      "Word: ACo_2O_4, Probability: 0.0032\n",
      "Word: Ba_3Mn_2Fe_24O_41, Probability: 0.0032\n",
      "Word: Co_0.0014Ni_0.65Fe_2.34O_4..01, Probability: 0.0032\n",
      "Word: Co_49Ni_21Ga_30, Probability: 0.0032\n",
      "Word: NdFe_12, Probability: 0.0031\n",
      "Word: Sr_1.52Eu_0.48VO_3.948, Probability: 0.0031\n",
      "Word: Ba_2CoTaO_6, Probability: 0.0031\n",
      "Word: Co_3GaN, Probability: 0.0030\n",
      "Word: Fe_2Mn_1Co_0.5, Probability: 0.0029\n",
      "Word: K_xNa_1-xFe_7O_11, Probability: 0.0028\n",
      "Word: Gd_0.1Fe_1.9O_4, Probability: 0.0028\n",
      "Word: CoB_7, Probability: 0.0027\n",
      "Word: MX_2Fe_4O_11, Probability: 0.0027\n",
      "Word: Ni_48Mn_36Sn_14Pd_2, Probability: 0.0026\n",
      "Word: Fe_3S_4, Probability: 0.0026\n",
      "Word: Ca_2FeAlO_5, Probability: 0.0026\n",
      "Word: ErMn_0.5Fe_0.5O_3, Probability: 0.0025\n",
      "Word: N, Probability: 0.0025\n",
      "Word: LiCoTiO_4, Probability: 0.0025\n",
      "Word: Tb_1.90Ni_0.10O_3, Probability: 0.0025\n",
      "Word: , Probability: 0.0025\n",
      "Word: Ni_55.7Ti, Probability: 0.0024\n",
      "Word: Zn_0.60Ni_0.40Fe_2O_4, Probability: 0.0024\n",
      "Word: Ti_2NiH_0.8, Probability: 0.0024\n",
      "Word: LiZn_0.5Mn_1.5O_4, Probability: 0.0024\n",
      "Word: RCo_5, Probability: 0.0023\n",
      "Word: Li_6MnCl_8, Probability: 0.0023\n",
      "Word: Dy_2Fe_10Al_7, Probability: 0.0023\n",
      "Word: FeGa_2Se_4, Probability: 0.0023\n",
      "Word: CoO_4, Probability: 0.0023\n",
      "Word: Ni_21Nb_2B_6, Probability: 0.0022\n",
      "Word: MnLu_2S_4, Probability: 0.0022\n",
      "Word: NiMnO_3, Probability: 0.0022\n",
      "Word: Fe_73.5Cu_1Nb_3Si_13.5B_9, Probability: 0.0021\n",
      "Word: CsV_3Sb_5, Probability: 0.0021\n",
      "Word: LaFe_11ZnO_19, Probability: 0.0021\n",
      "Word: LiNi_0.5Mn_0.3Co_0.2O_2, Probability: 0.0021\n",
      "Word: Co_xZn_7-xSb_2O_12, Probability: 0.0021\n",
      "Word: Si, Probability: 0.0020\n",
      "Word: MgIn_2-xMn_xO_4, Probability: 0.0020\n",
      "Word: Ba_2V_3O_9, Probability: 0.0020\n",
      "Word: Co_2TiAl, Probability: 0.0020\n",
      "Word: Cd_1-xFe_1, Probability: 0.0020\n",
      "Word: Fe_86Cr_6P_6C_2, Probability: 0.0020\n",
      "Word: Ni_5GeB_2O_10, Probability: 0.0019\n",
      "Word: BaFeC_3, Probability: 0.0019\n",
      "Word: Ni, Probability: 0.0019\n",
      "Word: NiNH_2SO_3, Probability: 0.0019\n",
      "Word: Ce_1-xY_xOFeAs, Probability: 0.0019\n",
      "Word: YCr_1-xFe_xO_3, Probability: 0.0018\n",
      "Word: Co_0.2Zn_0.8Fe_2O_4, Probability: 0.0018\n",
      "Word: Nd_0.6Sr_0.4MnO_3, Probability: 0.0018\n",
      "Word: Co_xZn_3-xNi_4Sb_2O_12, Probability: 0.0018\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetic damping constant in low temperture.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 100\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b97d150-810d-4cf9-879e-261861c030d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0fdf16af59424cbad4688074921883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-8400')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('results/checkpoint-8400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1717ae01-2ff2-4175-9d6d-0a4fe5f6365d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d23b94f46fc4fe887ebe7deb127d58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "original_embeddings = model.get_input_embeddings().weight.detach().clone()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-8400')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_adapter('results/checkpoint-8400')\n",
    "\n",
    "\n",
    "embeddings = model.get_input_embeddings().weight.data\n",
    "embeddings[:len(original_tokenizer)] = original_embeddings[:len(original_tokenizer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6c9a0-ea83-464d-b371-86fbecb8f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Magnetic materials with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material:\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetic damping constant.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "# 在计算softmax之前，为了数值稳定性，从logits中减去每个logit的最大值\n",
    "logits_stable = logits - torch.max(logits, dim=-1, keepdim=True).values\n",
    "\n",
    "probabilities = torch.softmax(logits_stable[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 10\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d878c63e-2a64-4790-a3a0-5ad01d25706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37553"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6db7b0b-f14a-4c1c-8cc5-765ffeca6795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   673,   278, 17279, 29901,    13,  3561,  1212,   293, 17279,\n",
       "           411,  4482, 15611,   270,  1160,   292,  4868, 29889,    13,   450,\n",
       "         13206, 16637,  7063,   310,   278,  5518, 29901]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae5a43c-8612-44c3-8b8a-ddc8e679b73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   673,   278, 17279, 29901,    13,  3561,  1212,   293, 17279,\n",
       "           411,  4482, 15611,   270,  1160,   292,  4868, 29889,    13,   450,\n",
       "         13206, 16637,  7063,   310,   278,  5518, 29901, 37551]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dda011c-9377-4412-af00-4297571242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('result', save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea844d33-6ef9-4ea0-b695-1518534a1dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('result/tokenizer_config.json',\n",
       " 'result/special_tokens_map.json',\n",
       " 'result/tokenizer.model',\n",
       " 'result/added_tokens.json',\n",
       " 'result/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1620556-3e89-48ab-9ac5-3baaea7775bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results/tokenizer_config.json',\n",
       " 'results/special_tokens_map.json',\n",
       " 'results/tokenizer.model',\n",
       " 'results/added_tokens.json',\n",
       " 'results/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601970e5-872e-4680-8244-01c41a211fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "291eb404-d313-4f1f-87c3-e489d24d60e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(38111, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=128, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=38111, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e506f375-179c-4782-a1c7-44079a380f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True base_model.model.model.embed_tokens.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.0.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.1.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.2.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.3.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.4.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.5.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.6.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.7.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.8.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.9.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.10.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.11.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.12.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.13.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.14.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.15.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.16.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.17.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.18.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.19.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.20.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.21.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.22.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.23.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.24.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.25.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.26.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.27.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.28.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.29.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.30.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.31.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.norm.weight torch.float32\n",
      "False base_model.model.lm_head.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(param.requires_grad, name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81fa8868-d7b2-4b52-9c5d-8d46a3d0af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: base_model.model.model.embed_tokens.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "# Verify which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346aa5df-43be-459d-98fc-247b0a2fda7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa5b3e-125c-4fb1-b073-82301ba20d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53006efd-ca7f-4cf7-b226-331f6f4316b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Trainable: {name}\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de8f185-07da-46c0-a3af-0f45f6b39016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7faf7c779ee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db0120f-b1a6-441a-8dbe-d0e59fe4e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38544, 4096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a34e1d06-cd59-4b28-9915-a865bb031c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6922694656"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d8bd59-e9dd-4a70-9e38-02944052fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38545, 4096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f10f6-db0d-47ad-9c4f-6d4cc888b013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
