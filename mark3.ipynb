{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a8afe9-1f35-48de-8384-6063d350cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340eccc8-60ac-4b8c-9807-dcbbc86185e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b402fd3c988479294ab9ed4dc96ec2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9148cc-49d4-4e74-bc10-1a8ef0788108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             load_in_8bit=True,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('enyuan/llama')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('enyuan/llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f68fc9-962f-4f5f-8c74-739fb4e2dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['publicationDate', 'title', 'abstract', 'doi'],\n",
      "    num_rows: 165071\n",
      "})\n",
      "Dataset({\n",
      "    features: ['title', 'abstract', 'publicationDate'],\n",
      "    num_rows: 559\n",
      "})\n",
      "The abstract of the paper:\n",
      " Inconel 625 sustainable milling surface integrity and the dependence on alloy processing route\n",
      " Abstract: The discovery of deepwater oil and gas sources has altered the scenario of world production of oil products, attracting even more attention to nickel superalloys. However, this class of materials can be used in several applications. Furthermore, nickel superalloys are highly dependent on their processing history, and the manner in which superalloys react to machining can directly affect the finished product. This work aims to evaluate the surface integrity of two different materials after cryogenic side-milling in conditions that stimulate severe plastic deformation (SPD) and high heat generation. The results show that the material response to machining depends strongly on the pre-processing route instead of most assumptions. While cryogenic cooling led to significant sub-surface hardness and microstructural changes in wrought Inconel 625 alloy, such changes were not observed for clad Inconel 625. Therefore, in order to achieve significant surface integrity changes, process parameters need to be selected and optimized accordingly. Also, the findings indicate that some new factors established significant affect/change surface integrity: (a) SPD through a high r_β/h ratio; (b) the specific pre-processing thermomechanical history of the workpiece material; and (c) and cryogenic cooling, by changing material properties, reducing temperature and altering cutting phenomena and chip formation. </s> \n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"enyuan/Abstracts\")\n",
    "data_train = data[\"train\"]\n",
    "\n",
    "custom_data = load_dataset('json', data_files='data_eval.json')\n",
    "data_val = custom_data['train']\n",
    "\n",
    "# Print the dataset details\n",
    "print(data_train)\n",
    "print(data_val)\n",
    "\n",
    "# Access an example\n",
    "#example = data_train[0]\n",
    "#print(example)\n",
    "\n",
    "def generate_prompt(title, abstract=None, eos_token=\"</s>\"):\n",
    "  instruction = \"The abstract of the paper:\\n\"\n",
    "  input = f\"{title}\\n\"\n",
    "  abstract = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, abstract])\n",
    "  return prompt\n",
    "\n",
    "print(generate_prompt(data_train[0][\"title\"], data_train[0][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347f25a1-b25f-40a6-9f81-4c0bdcfe8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract of the paper:\n",
      " Effect of cryogenic cooling on residual stresses and surface finish of 316L during hybrid manufacturing\n",
      " Abstract:   In this work, the effect of cryogenic cooling (CC) on residual stress and surface roughness is investigated in a hybrid additive manufactured part. A 3D printed stainless steel (SS) component was subjected to CC treatment using liquid nitrogen (LN2). Residual stress measurements were carried out by X-ray diffraction method and surface roughness analysis was performed with white light interferometry technique. The results show that the residual stress profile decreases significantly after CC treatment. Surface roughness also reduces considerably due to the removal of oxide layer formed at the interface between LN2 and SS. This study demonstrates that CC can be used as an effective post processing step for improving mechanical properties and surface quality of AM parts.\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[50][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70887e88-ddc1-44b4-848b-fd554027b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('materials.txt', 'r') as file:\n",
    "    word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b263087-4b39-4559-a1ca-fc0db0117772",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'title': word_list,\n",
    "    'abstract': [s.replace('_', '') for s in word_list],\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])\n",
    "\n",
    "new_data = {\n",
    "    'title': [s.replace('_', '') for s in word_list],\n",
    "    'abstract': word_list,\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a51116-f6ef-4701-939f-d07933fb8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_train.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644d9dfc-ad91-4d3e-bb14-2ee5600631c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The material :\n",
      " NiFeAlO4 is NiFeAlO_4 </s> \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(type, title, abstract=None, eos_token=\"</s>\"):\n",
    "    if type == 'material':\n",
    "        instruction = \"The material :\\n\"\n",
    "        input = f\"{title} is\"\n",
    "        output = f\"{abstract + ' ' + eos_token if abstract else ''} \"\n",
    "        prompt = (\" \").join([instruction, input, output])\n",
    "    else:\n",
    "        instruction = \"The abstract of the paper:\\n\"\n",
    "        input = f\"{title}\\n\"\n",
    "        output = f\"Abstract: {abstract + ' ' + eos_token if abstract else ''} \"\n",
    "        prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data_train[-1][\"doi\"], data_train[-1][\"title\"], data_train[-1][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4ce5ac-170a-42c2-8b05-b204b6bf9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bb07ccc-4f63-44d0-8e25-16ad278ff7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6110 tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add new tokens to the tokenizer\n",
    "num_added_toks = tokenizer.add_tokens(word_list)\n",
    "print(f\"Added {num_added_toks} tokens\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f119b5df-ac5a-46f8-be81-b12943b162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Freeze all parameters in the model\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# Enable gradient updates for the entire embedding layer\n",
    "# Assuming you might want to fine-tune all embeddings, but here's how to selectively unfreeze\n",
    "embeddings.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8483b140-a7e6-4175-8e01-6e3f13879301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',            # 输出目录\n",
    "    num_train_epochs=1,              # 总训练轮数\n",
    "    per_device_train_batch_size=4,   # 训练的batch size\n",
    "    per_device_eval_batch_size=4,    # 验证的batch size\n",
    "    gradient_accumulation_steps=4, \n",
    "    gradient_checkpointing=True,\n",
    "    optim = \"paged_adamw_32bit\",\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    warmup_steps=10,                # 预热步数\n",
    "    learning_rate = 1e-4,\n",
    "    max_grad_norm = 0.2,\n",
    "    #max_steps = 50,\n",
    "    #warmup_ratio = 0.03,\n",
    "    #weight_decay=0.01,               # 权重衰减\n",
    "    save_strategy=\"steps\",           # 设置保存策略为\"steps\"\n",
    "    save_steps=300,                  # 每500步保存一次模型\n",
    "    save_total_limit=3,              # 最多保存3个检查点\n",
    "    evaluation_strategy=\"steps\",     # 设置评估策略为\"steps\"\n",
    "    group_by_length=True,\n",
    "    eval_steps=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e64ad6-36bd-4b68-b995-bfef056bb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d49402b-1027-4e8b-a777-54b6b03f7966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb45021573248b2a934cb91955ea857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be845e8b754c4fbb9f1b1a7510fa77da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 47.54 GiB total capacity; 45.97 GiB already allocated; 85.12 MiB free; 46.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m     25\u001b[0m         module \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 331\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2924\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2925\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:817\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py:805\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1083\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1082\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1168\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1165\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1168\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1008\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    997\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    998\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    999\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         cache_position,\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:749\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    748\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    752\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:236\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    234\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB (GPU 0; 47.54 GiB total capacity; 45.97 GiB already allocated; 85.12 MiB free; 46.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for a, d, s in zip(prompt[\"doi\"], prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(a, d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50c387-2084-4e3b-b5d6-6ac9a6d15854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db7b0b-f14a-4c1c-8cc5-765ffeca6795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ae5a43c-8612-44c3-8b8a-ddc8e679b73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['abstract', 'doi', 'publicationDate', 'title'],\n",
       "    num_rows: 189515\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dda011c-9377-4412-af00-4297571242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('result', save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea844d33-6ef9-4ea0-b695-1518534a1dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('result/tokenizer_config.json',\n",
       " 'result/special_tokens_map.json',\n",
       " 'result/tokenizer.model',\n",
       " 'result/added_tokens.json',\n",
       " 'result/tokenizer.json')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1620556-3e89-48ab-9ac5-3baaea7775bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results/tokenizer_config.json',\n",
       " 'results/special_tokens_map.json',\n",
       " 'results/tokenizer.model',\n",
       " 'results/added_tokens.json',\n",
       " 'results/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601970e5-872e-4680-8244-01c41a211fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e506f375-179c-4782-a1c7-44079a380f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True base_model.model.model.embed_tokens.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.0.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.1.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.2.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.3.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.4.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.5.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.6.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.7.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.8.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.9.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.10.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.11.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.12.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.13.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.14.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.15.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.16.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.17.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.18.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.19.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.20.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.21.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.22.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.23.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.24.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.25.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.26.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.27.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.28.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.29.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.30.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.input_layernorm.weight torch.float32\n",
      "False base_model.model.model.layers.31.post_attention_layernorm.weight torch.float32\n",
      "False base_model.model.model.norm.weight torch.float32\n",
      "False base_model.model.lm_head.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(param.requires_grad, name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81fa8868-d7b2-4b52-9c5d-8d46a3d0af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: base_model.model.model.embed_tokens.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "# Verify which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346aa5df-43be-459d-98fc-247b0a2fda7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa5b3e-125c-4fb1-b073-82301ba20d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53006efd-ca7f-4cf7-b226-331f6f4316b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Trainable: {name}\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de8f185-07da-46c0-a3af-0f45f6b39016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7faf7c779ee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db0120f-b1a6-441a-8dbe-d0e59fe4e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38544, 4096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a34e1d06-cd59-4b28-9915-a865bb031c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6922694656"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d8bd59-e9dd-4a70-9e38-02944052fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38545, 4096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f10f6-db0d-47ad-9c4f-6d4cc888b013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
