{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e470cd8-840a-4127-b36d-090297fd66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import argparse\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "        get_peft_model, \n",
    "        prepare_model_for_kbit_training, \n",
    "        LoraConfig\n",
    "    )\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4670ff28-27fb-473c-b53f-ec5bdbc147b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b389c969920c4ce098e3c08460a88e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2ee7856cfa4e73b8df2e61c9b29d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c49d05829747d6ac118a2d9367d523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502259db84c84154add5c24597b35f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7674f7b62d504babac8b58a09378e713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecc40d7815944d3b3df37583de9db38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ab63b923dc4488956d074229656775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487aa7ea613942e8a94c531db2ba7033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d993841864946a8bc1b2857df0044b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "414c8c9045f74511a91073f39a3bd5c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61de1f0905cf4c0e9320cca262e98505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            load_in_8bit=True,\n",
    "                                            device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60c7954-dfd8-414c-9ef1-0c3e20204835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abddad1d88bd4457b0ffe710da03022e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12cb720c9f243b79252652004765de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb167a2bcd1b4da08254a5bf4ce14d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0908b7a782b5496ea046c95b06863f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e758a17fa546639ce4a2da2b0f4291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018d99a7c4ef4b16bf405ea9f0d8760a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler.pt:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0277a335c1c34688bbb0968ffb85d637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5526be6aad4a6b8cfbfd701e45ec9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce0f74a17084f8fbe01ee269a6b98ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6e6757e9304d0ab3a42f3462c910c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e932fcdf8f4719ab2a7e1170e0de6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/537M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d9f00dfbed45bb9f80586e0811facf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/3.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2a73da480840729c1e6035e1eff44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "optimizer.pt:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8dc599ffd44926b5f04cca7e7d882c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd58916e7d74313a3a25899b4de09f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'cp/models--enyuan--llama/snapshots/cc9f2b840050248a80e02c0a61e3cc0050c54b2e'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_name = \"enyuan/llama\"\n",
    "#output_dir = \"cp\"\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=\"enyuan/llama\", cache_dir=\"cp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58bb3669-59fc-4844-978f-d31948567a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(materials, gilbert=None, eos_token=\"</s>\"):\n",
    "  instruction = \"Answer the value:\\n\"\n",
    "  input = f\"Gilbert damping constant of {materials}\\n\"\n",
    "  gilbert = f\"Value: {gilbert + ' ' + eos_token if gilbert else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, gilbert])\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddc843b2-7b3e-4d9c-bc64-ec08e63fc9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the value:\n",
      " Gilbert damping constant of Fe\n",
      " Value:  \n",
      "Word: 1, Probability: 0.3206\n",
      "Word: 0, Probability: 0.2807\n",
      "Word: 2, Probability: 0.0810\n",
      "Word: 5, Probability: 0.0455\n",
      "Word: 4, Probability: 0.0448\n",
      "Word: 3, Probability: 0.0441\n",
      "Word: 6, Probability: 0.0303\n",
      "Word: 7, Probability: 0.0203\n",
      "Word: 9, Probability: 0.0197\n",
      "Word: 8, Probability: 0.0194\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('Fe')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 10\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9b843b-457f-4aea-b951-5f37c16971c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the value:\n",
      " Gilbert damping constant of Fe\n",
      " Value:  1.079×10−3 N⋅m/rad\n",
      "\n",
      "### 2.\n",
      "\n",
      "The following is a list of values for the Gilbert damping constant, $k_g$, of iron (Fe).\n",
      "\n",
      "Answer the value: Gilbert damping constant of Iron\n",
      " Value:   4.685×10−3 N⋅m/rad\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('Fe')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=1000,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d54db31b-59b9-473f-9265-4142818dfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model_id = \"cp/models--enyuan--llama/snapshots/cc9f2b840050248a80e02c0a61e3cc0050c54b2e\"\n",
    "peft_model = PeftModel.from_pretrained(model, peft_model_id, torch_dtype=torch.float16, offload_folder=\"lora_results/lora_7/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0f73bb9-2a2c-4f20-a8b1-537889ea2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the value:\n",
      " Gilbert damping constant of Fe\n",
      " Value:  0.0135\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('Fe')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7bc86a4-ec15-4a55-9979-68666af10175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the value:\n",
      " Gilbert damping constant of Fe\n",
      " Value:  \n",
      "Word: 0, Probability: 0.3416\n",
      "Word: </s>, Probability: 0.1380\n",
      "Word: 1, Probability: 0.1257\n",
      "Word: 2, Probability: 0.0452\n",
      "Word: γ, Probability: 0.0408\n",
      "Word: 4, Probability: 0.0315\n",
      "Word: \n",
      ", Probability: 0.0292\n",
      "Word: 5, Probability: 0.0213\n",
      "Word: 3, Probability: 0.0204\n",
      "Word: ξ, Probability: 0.0155\n"
     ]
    }
   ],
   "source": [
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = peft_model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 10\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79bd9991-6ef0-44f6-87cd-a2491b89118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(materials, gilbert=None, eos_token=\"</s>\"):\n",
    "  instruction = \"Answer the materials:\\n\"\n",
    "  input = f\"Metal oxide with {materials}\\n\"\n",
    "  gilbert = f\"Materials: {gilbert + ' ' + eos_token if gilbert else ''} \"\n",
    "  prompt = (\" \").join([instruction, input, gilbert])\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58999089-f4c8-46c9-8206-bc2c38bb8fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Metal oxide with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material: 1\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e92aa74-7c5e-49a1-94d3-dd79b8bf6b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Material with low magnetic damping constant.\n",
      " The molecular formula of the material: C_1\n",
      "Word: 2, Probability: 0.3803\n",
      "Word: 0, Probability: 0.1912\n",
      "Word: 8, Probability: 0.1040\n",
      "Word: 6, Probability: 0.0797\n",
      "Word: 4, Probability: 0.0651\n",
      "Word: 5, Probability: 0.0491\n",
      "Word: 1, Probability: 0.0461\n",
      "Word: 3, Probability: 0.0420\n",
      "Word: 7, Probability: 0.0221\n",
      "Word: 9, Probability: 0.0159\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(prompt, output=None, eos_token=\"</s>\"):\n",
    "    instruction = \"Answer the materials:\\n\"\n",
    "    input = f\"Material with {prompt}\\n\"\n",
    "    output = f\"The molecular formula of the material: C_1\"\n",
    "    prompt = (\" \").join([instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "input_prompt = generate_prompt('low magnetic damping constant.')\n",
    "print(input_prompt)\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = peft_model(input_tokens).logits\n",
    "\n",
    "probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "# Get the top 10 token IDs and their probabilities\n",
    "top_k = 10\n",
    "top_probabilities, top_token_ids = torch.topk(probabilities, top_k)\n",
    "\n",
    "# Convert probabilities to a human-readable format (e.g., Python list)\n",
    "top_probabilities = top_probabilities.squeeze().tolist()\n",
    "top_token_ids = top_token_ids.squeeze().tolist()\n",
    "\n",
    "# Decode each token ID and pair it with its probability\n",
    "top_words_with_probs = [(tokenizer.decode([token_id]), prob) for token_id, prob in zip(top_token_ids, top_probabilities)]\n",
    "\n",
    "# Display the results\n",
    "for word, prob in top_words_with_probs:\n",
    "    print(f\"Word: {word}, Probability: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4708b9c7-c5e2-47df-9bfb-78d0ea7123f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Metal oxide with low magnetic damping constant.\n",
      " Materials:  \n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('low magnetic damping constant.')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23c81787-1398-4c67-a32e-743ce5765ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the materials:\n",
      " Metal oxide with low density of states at the Fermi level\n",
      " Materials:  \n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('low density of states at the Fermi level')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8719804a-eb44-4f3b-aec8-67493910dfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List five materials:\n",
      " Alloys with low Gilbert damping constant. e.g. low magnetic damping constant. e.g. \n",
      " Materials:  \n",
      "$${Co-Pd}$$ Co - Pd , $${Ni-Fe}$$ Ni - Fe , $${NbTa}$$ NbTa , $${Cu-Ag}$$ Cu - Ag , $${Au-Ag}$$ Au - Ag . \n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('low magnetic damping constant. e.g. ')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3003fa0f-1650-40c6-a6b7-d109174bd1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List five materials:\n",
      " Metal oxide with low density of states at the Fermi level\n",
      " Materials:  \n",
      "$$\\mathrm{MgO}$$ M g O , $$ \\mathrm{Al_2O_3} $$ Al 2 O 3 , $$ \\mathrm{SiO_2} $$ SiO 2 , $$ \\mathrm{Ta_2O_5} $$ Ta 2 O 5 .  \n",
      "$$\\mathrm{CuO}$$ C u O , $$ \\mathrm{ZnO} $$ Z n O , $$ \\mathrm{Nb\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('low density of states at the Fermi level')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.3,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "999e53a9-bc64-4e95-932d-a471d40cd1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List five materials:\n",
      " Metal oxide with low magnetic damping constant. e.g. \n",
      " Materials:  \n",
      "$${\\text{MnBi}}_{2}{\\text{O}}_{4}$$ MnBi 2 O 4 , $${\\text{CuTiO}}_{3}$$ CuTiO 3 .  \n",
      "Materials:  \n",
      "$${\\text{Fe}}_{3} {\\text{O}}_{4}$$ Fe 3 O 4 , $${\\text{Nd}_{1 - x}Dy_{x}} {\\text{FeO}}_{4}$$ Nd\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt('low magnetic damping constant. e.g. ')\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    generation_output = peft_model.generate(\n",
    "        input_ids=input_tokens,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.9,\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.15,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "      )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f33fd7-008c-4d74-82d0-4814b21b696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "# this should be set for finutning and batched inference\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Loading in 8 bit ...\"\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9981f016-a05d-4ea9-a7f2-1c8f69f1597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"cp\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "per_device_eval_batch_size = 4\n",
    "eval_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 500\n",
    "logging_steps = 500\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 0.2\n",
    "#max_steps = 50\n",
    "warmup_ratio = 0.03\n",
    "evaluation_strategy=\"epoch\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            logging_steps=logging_steps,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            #max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49676da0-6985-4bcb-a80f-b278b283c074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa36c0c5a7e41ae9aedc02dd08af26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2383540a8fcf4a3b8b775cb070b6e7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 08:49, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.098400</td>\n",
       "      <td>1.891765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.891100</td>\n",
       "      <td>1.811098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.814300</td>\n",
       "      <td>1.744567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.705700</td>\n",
       "      <td>1.667625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>1.575743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for d, s in zip(prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b1b0f0-b8db-4e73-bd5d-ef09466dbf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should be set for finutning and batched inference\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee3021-a7c8-419c-8174-b55fc182547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in 8 bit ...\"\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "#model = get_peft_model(model, lora_config)\n",
    "\n",
    "output_dir = \"cp\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "per_device_eval_batch_size = 4\n",
    "eval_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 200\n",
    "logging_steps = 200\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 0.2\n",
    "#max_steps = 50\n",
    "warmup_ratio = 0.03\n",
    "evaluation_strategy=\"epoch\"\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            optim=optim,\n",
    "            evaluation_strategy=evaluation_strategy,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            logging_steps=logging_steps,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            #max_steps=max_steps,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            group_by_length=True,\n",
    "            lr_scheduler_type=lr_scheduler_type,\n",
    "            ddp_find_unused_parameters=False,\n",
    "            eval_accumulation_steps=eval_accumulation_steps,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        )\n",
    "\n",
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for d, s in zip(prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    #peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1669c18-e142-4baf-893e-5f0c9f1827d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
